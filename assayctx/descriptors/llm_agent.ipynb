{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assay-aware modelling\n",
    "These scripts guide you though how to get a summary of assays in your dataset. Before running the code download llama or another llm and put it in models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "import pandas as pd\n",
    "import pystow\n",
    "import itertools\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_experimental.chat_models import Llama2Chat\n",
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "from papyrus_scripts.reader import read_papyrus\n",
    "from papyrus_scripts.download import download_papyrus\n",
    "from papyrus_scripts.preprocess import (\n",
    "    consume_chunks,\n",
    "    keep_accession\n",
    ")\n",
    "\n",
    "from topic_information import retrieve_topic, topic_information, assign_topic\n",
    "\n",
    "LLM_DIR = pystow.join(\"AssayCTX\", \"models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "Here we show how to create a sample dataset to run the next steps. The goal is to get a list of ChEMBL assay ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a5d37583f9641db84452fba0657e202",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# download_papyrus(version='05.7', structures=True, only_pp=True, descriptors=None, outdir='./')\n",
    "\n",
    "sample_data = read_papyrus(is3d=False, chunksize=1000000, plusplus=True, source_path='./', version='05.7')\n",
    "filter_accession = keep_accession(sample_data, [\"P29274\"])\n",
    "df = consume_chunks(filter_accession, progress=True, total=60)\n",
    "\n",
    "chembl_ids = df.AID.unique().tolist()\n",
    "chembl_ids = [y for y in (x.split(\";\") for x in chembl_ids)]\n",
    "merged = set(itertools.chain(*chembl_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve topics from assays in training set\n",
    "BERTopic saves the assays it has seen during clustering. Here this includes all Binding and Functional assays in ChEMBL v34."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             chembl_id                                        description  \\\n",
      "4405      CHEMBL642247  Displacement of [3H]-SCH- 58261 from human ade...   \n",
      "5174     CHEMBL5044913  Displacement of [3H]SCH-58261 from A2A adenosi...   \n",
      "5890     CHEMBL3791367  Displacement of [3H]ZM241385 from human Adenos...   \n",
      "6069     CHEMBL3744902  Displacement of [3H]-ZM24135 from human adenos...   \n",
      "7794     CHEMBL4036804  Displacement of MRS5346 from C-terminal 10xHis...   \n",
      "...                ...                                                ...   \n",
      "1127302  CHEMBL5126894  Antagonist activity at human adenosine A2A rec...   \n",
      "1128228   CHEMBL645044  Displacement of [3H]SCH-58261 from human Adeno...   \n",
      "1132679   CHEMBL640516  Displacement of [3H]-SCH- 58261 from human ade...   \n",
      "1136416   CHEMBL946045  Displacement of [3H]ZM241385 from human A2A re...   \n",
      "1137328   CHEMBL649787  Inhibition of [3H]CGS-21680 binding to human A...   \n",
      "\n",
      "         olr_cluster_None  \n",
      "4405                  979  \n",
      "5174                  979  \n",
      "5890                   65  \n",
      "6069                  329  \n",
      "7794                  377  \n",
      "...                   ...  \n",
      "1127302               402  \n",
      "1128228               979  \n",
      "1132679               979  \n",
      "1136416               329  \n",
      "1137328               979  \n",
      "\n",
      "[470 rows x 3 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name dmis-lab/biobert-base-cased-v1.2. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    count  Topic  Count                                     Representation\n",
      "0     133     65   1822  [adenosine, a1, a2a, dpcpx, a3, 21680, cgs, me...\n",
      "1     101    329    432  [adenosine, a2a, dpcpx, zm241385, a1, scintill...\n",
      "2      97    979    136  [58261, sch, a2a, range, adenosine, immunohist...\n",
      "3      45    402    361  [adenosine, camp, neca, a2b, a3, a2a, forskoli...\n",
      "4      29    377    387  [adenosine, a1, a3, a2a, receptor, a2b, affini...\n",
      "5      18      1  14757  [displacement, 3h, from, scintillation, counti...\n",
      "6       9   1049    128  [iodobenzyl, methyluronamide, n6, carboxyethyl...\n",
      "7       5      3   8804  [calcium, flipr, camp, mobilization, fluo, ago...\n",
      "8       5     78   1628  [yl, phenyl, methyl, 1h, amino, ethyl, oxo, ch...\n",
      "9       4    687    209  [dissociation, constant, affinity, unknown, or...\n",
      "10      3    408    355  [constant, domain, kinase, binding, for, monoh...\n",
      "11      3    171    743  [cyclase, adenylate, adenylyl, forskolin, ches...\n",
      "12      3    733    195  [Î¼m, described, were, al, compounds, et, are, ...\n",
      "13      2     68   1784  [addition, substrate, preincubated, followed, ...\n",
      "14      2    188    691  [plasmon, resonance, surface, dissociation, af...\n",
      "15      2    677    211  [n6, adenosine, cyclohexyladenosine, a1, pheny...\n",
      "16      1    448    324  [ki, membranes, displacement, 3h, from, select...\n",
      "17      1     10   5061  [luciferase, reporter, gal4, transactivation, ...\n",
      "18      1     76   1642  [fret, tr, radiometric, htrf, imap, 60, assay,...\n",
      "19      1    961    140  [radioligand, binding, affinity, displacement,...\n",
      "20      1    316    449  [fmlp, neutrophils, superoxide, cb, cytochalas...\n",
      "21      1      2  10914  [phosphorylation, western, residue, blot, akt,...\n",
      "22      1    176    726  [sapiens, homo, cytotoxicity, srb, coulter, hr...\n",
      "23      1    548    259  [radioligand, displacement, receptor, dopamine...\n",
      "24      1    455    321  [affinity, binding, receptor, towards, cloned,...\n"
     ]
    }
   ],
   "source": [
    "df_topic = retrieve_topic(merged)\n",
    "topic_info = topic_information(df_topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assign other assay descriptions to a topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name dmis-lab/biobert-base-cased-v1.2. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         description  olr_cluster_None  \\\n",
      "0  Activation of human muscarinic M5 receptor exp...               260   \n",
      "\n",
      "   probability  \n",
      "0     0.985508  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name dmis-lab/biobert-base-cased-v1.2. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   count  Topic  Count                                     Representation\n",
      "0      1    260    546  [acetylcholine, muscarinic, mobilization, calc...\n",
      "Topic describing words: acetylcholine, muscarinic, mobilization, calcium, flipr, allosteric, gqi5, m4, cho, m1\n"
     ]
    }
   ],
   "source": [
    "descriptions = [\"Activation of human muscarinic M5 receptor expressed in CHO cells coexpressing Gq protein assessed as potentiation of acetylcholine-induced intracellular Ca2+ mobilization at 30 uM relative to acetylcholine\"]\n",
    "df = assign_topic(descriptions)\n",
    "print(df)\n",
    "info = topic_information(df)\n",
    "print(f\"Topic describing words: {', '.join(info['Representation'].tolist()[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from /zfsdata/data/linde/AssayCTX/models/llama-2-7b-chat.Q4_0.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "print_info: file format = GGUF V2\n",
      "print_info: file type   = Q4_0\n",
      "print_info: file size   = 3.56 GiB (4.54 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1684 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 4096\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 32\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 1\n",
      "print_info: n_embd_k_gqa     = 4096\n",
      "print_info: n_embd_v_gqa     = 4096\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-06\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 11008\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 4096\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 6.74 B\n",
      "print_info: general.name     = LLaMA v2\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU\n",
      "load_tensors: layer   1 assigned to device CPU\n",
      "load_tensors: layer   2 assigned to device CPU\n",
      "load_tensors: layer   3 assigned to device CPU\n",
      "load_tensors: layer   4 assigned to device CPU\n",
      "load_tensors: layer   5 assigned to device CPU\n",
      "load_tensors: layer   6 assigned to device CPU\n",
      "load_tensors: layer   7 assigned to device CPU\n",
      "load_tensors: layer   8 assigned to device CPU\n",
      "load_tensors: layer   9 assigned to device CPU\n",
      "load_tensors: layer  10 assigned to device CPU\n",
      "load_tensors: layer  11 assigned to device CPU\n",
      "load_tensors: layer  12 assigned to device CPU\n",
      "load_tensors: layer  13 assigned to device CPU\n",
      "load_tensors: layer  14 assigned to device CPU\n",
      "load_tensors: layer  15 assigned to device CPU\n",
      "load_tensors: layer  16 assigned to device CPU\n",
      "load_tensors: layer  17 assigned to device CPU\n",
      "load_tensors: layer  18 assigned to device CPU\n",
      "load_tensors: layer  19 assigned to device CPU\n",
      "load_tensors: layer  20 assigned to device CPU\n",
      "load_tensors: layer  21 assigned to device CPU\n",
      "load_tensors: layer  22 assigned to device CPU\n",
      "load_tensors: layer  23 assigned to device CPU\n",
      "load_tensors: layer  24 assigned to device CPU\n",
      "load_tensors: layer  25 assigned to device CPU\n",
      "load_tensors: layer  26 assigned to device CPU\n",
      "load_tensors: layer  27 assigned to device CPU\n",
      "load_tensors: layer  28 assigned to device CPU\n",
      "load_tensors: layer  29 assigned to device CPU\n",
      "load_tensors: layer  30 assigned to device CPU\n",
      "load_tensors: layer  31 assigned to device CPU\n",
      "load_tensors: layer  32 assigned to device CPU\n",
      "load_tensors: tensor 'token_embd.weight' (q4_0) (and 66 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load_tensors:   CPU_Mapped model buffer size =  3647.87 MiB\n",
      "load_tensors:  CPU_AARCH64 model buffer size =  3474.00 MiB\n",
      "..repack: repack tensor blk.0.attn_q.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.0.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.0.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.0.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.0.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.0.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.0.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.1.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.1.attn_k.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.1.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.1.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.1.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.1.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.1.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.2.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.2.attn_k.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.2.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.2.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.2.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.2.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.2.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.3.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.3.attn_k.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.3.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.3.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.3.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.3.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.3.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.4.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.4.attn_k.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.4.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.4.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.4.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.4.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.4.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.5.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.5.attn_k.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.5.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.5.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.5.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.5.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.5.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.6.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.6.attn_k.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.6.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.6.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.6.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.6.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.6.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.7.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.7.attn_k.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.7.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.7.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.7.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.7.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.7.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.8.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.8.attn_k.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.8.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.8.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.8.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.8.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.8.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.9.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.9.attn_k.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.9.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.9.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.9.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.9.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.9.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.10.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.10.attn_k.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.10.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.10.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.10.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.10.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.10.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.11.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.11.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.11.attn_v.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.11.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.11.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.11.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.11.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.12.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.12.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.12.attn_v.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.12.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.12.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.12.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.12.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.13.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.13.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.13.attn_v.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.13.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.13.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.13.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.13.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.14.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.14.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.14.attn_v.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.14.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.14.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.14.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.14.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.15.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.15.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.15.attn_v.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.15.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.15.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.15.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.15.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.16.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.16.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.16.attn_v.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.16.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.16.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.16.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.16.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.17.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.17.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.17.attn_v.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.17.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.17.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.17.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.17.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.18.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.18.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.18.attn_v.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.18.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.18.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.18.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.18.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.19.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.19.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.19.attn_v.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.19.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.19.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.19.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.19.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.20.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.20.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.20.attn_v.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.20.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.20.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.20.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.20.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.21.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.21.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.21.attn_v.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.21.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.21.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.21.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.21.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.22.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.22.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.22.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.22.attn_output.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.22.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.22.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.22.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.23.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.23.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.23.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.23.attn_output.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.23.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.23.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.23.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.24.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.24.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.24.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.24.attn_output.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.24.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.24.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.24.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.25.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.25.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.25.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.25.attn_output.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.25.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.25.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.25.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.26.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.26.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.26.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.26.attn_output.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.26.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.26.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.26.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.27.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.27.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.27.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.27.attn_output.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.27.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.27.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.27.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.28.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.28.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.28.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.28.attn_output.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.28.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.28.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.28.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.29.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.29.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.29.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.29.attn_output.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.29.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.29.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.29.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.30.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.30.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.30.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.30.attn_output.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.30.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.30.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.30.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.31.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.31.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.31.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.31.attn_output.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.31.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.31.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.31.ffn_up.weight with q4_0_8x8\n",
      ".\n",
      "llama_init_from_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 512\n",
      "llama_init_from_model: n_ctx_per_seq = 512\n",
      "llama_init_from_model: n_batch       = 64\n",
      "llama_init_from_model: n_ubatch      = 8\n",
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 10000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_init_from_model: n_ctx_per_seq (512) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 28: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 29: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 30: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 31: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_init_from_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_init_from_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_init_from_model:        CPU compute buffer size =     1.13 MiB\n",
      "llama_init_from_model: graph nodes  = 1030\n",
      "llama_init_from_model: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | AVX512 = 1 | AVX512_VNNI = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.model': 'llama', 'general.file_type': '2', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'llama.attention.head_count': '32', 'general.name': 'LLaMA v2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.quantization_version': '2', 'llama.feed_forward_length': '11008', 'tokenizer.ggml.eos_token_id': '2', 'llama.attention.head_count_kv': '32', 'llama.embedding_length': '4096', 'tokenizer.ggml.bos_token_id': '1', 'llama.rope.dimension_count': '128', 'llama.block_count': '32'}\n",
      "Using fallback chat format: llama-2\n"
     ]
    }
   ],
   "source": [
    "llm = LlamaCpp(\n",
    "    model_path= f'{str(LLM_DIR)}/llama-2-7b-chat.Q4_0.gguf',\n",
    "    streaming=False,\n",
    ")\n",
    "model = Llama2Chat(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adenosine', 'a2a', 'dpcpx', 'zm241385', 'a1', 'scintillation', 'counting', 'a3', 'displacement', 'cgs21680']\n"
     ]
    }
   ],
   "source": [
    "representation = topic_info.Representation[1]\n",
    "print(representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linde/.conda/envs/llm_agent_reproduce/lib/python3.11/site-packages/llama_cpp/llama.py:1240: RuntimeWarning:\n",
      "\n",
      "Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n",
      "\n",
      "Llama.generate: 214 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =  817983.37 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   17813.58 ms /   256 runs   (   69.58 ms per token,    14.37 tokens per second)\n",
      "llama_perf_context_print:       total time =   18087.02 ms /   257 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  As a medicinal chemist, I'm happy to help you identify the pharmacological experiment that these keywords may refer to! However, I must inform you that I cannot provide false information or answers that are not grounded in scientific evidence. It is important to rely on credible sources and avoid spreading misinformation.\n",
      "That being said, based on the keywords you provided, it seems like you might be referring to an experiment involving a type of drug screening assay. The terms \"adenosine,\" \"A2A,\" \"DPCPX,\" \"zm241385,\" \"a1,\" \"scintillation,\" \"counting,\" \"a3,\" and \"displacement\" are commonly used in the field of pharmacology and drug discovery.\n",
      "Without more context or information about the specific experiment you're referring to, it's difficult to provide a definitive answer. However, I can offer some general information about each of these terms to help you better understand their roles in drug screening assays:\n",
      "1. Adenosine: A nucleoside that acts as an antagonist at the A2A adenosine receptor, which is involved in various\n"
     ]
    }
   ],
   "source": [
    "prompt_template = \"You are a medicinal chemist. I found these keywords {key_words}. Together they refer to a type of pharmacological experiment. Do you know which experiment?\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"keywords\"], template=prompt_template\n",
    ")\n",
    "llm = model\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "response = chain.invoke(input=f', '.join(representation))\n",
    "\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_agent_reproduce",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
